{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torchinfo\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "from PIL import Image\n",
    "import skimage\n",
    "from tqdm import tqdm\n",
    "\n",
    "# importing a module with utilities for displaying stats and data\n",
    "import sys\n",
    "sys.path.insert(1, '../../util')\n",
    "import vcpi_util\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, epochs, loss_fn, optimizer, scheduler, early_stopper, save_prefix = 'model'):\n",
    "\n",
    "    history = {}\n",
    "\n",
    "    history['val_loss'] = []\n",
    "    history['loss'] = []\n",
    "    best_val_loss = np.inf\n",
    "\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        model.train()\n",
    "        start_time = time.time() \n",
    "        correct = 0\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, _) in tqdm(enumerate(train_loader, 0)):\n",
    "            \n",
    "            inputs = inputs.to(device)\n",
    "    \n",
    "            outputs = model(inputs)\n",
    "    \n",
    "            loss = loss_fn(outputs, inputs)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.cpu().detach().numpy()\n",
    "     \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i,_ in val_loader:\n",
    "                i = i.to(device)\n",
    "                o = model(i)\n",
    "                \n",
    "                #with torch.no_grad():\n",
    "                val_loss += loss_fn(o, i).cpu().detach().numpy()\n",
    "\n",
    "\n",
    "        old_lr = optimizer.param_groups[0]['lr']\n",
    "        scheduler.step(val_loss)\n",
    "        new_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        if old_lr != new_lr:\n",
    "            print('==> Learning rate updated: ', old_lr, ' -> ', new_lr)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        stop_time = time.time()\n",
    "        print(f'Epoch: {epoch:03d}; Loss: {epoch_loss:0.6f}; Val Loss: {val_loss:0.6f}; Elapsed time: {(stop_time - start_time):0.4f}')\n",
    "\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['loss'].append(epoch_loss)\n",
    " \n",
    "        ###### Saving ######\n",
    "        if val_loss < best_val_loss:\n",
    "           \n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model':model.state_dict(),\n",
    "                'history': history,\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'scheduler': scheduler.state_dict()\n",
    "                },\n",
    "                f'{save_prefix}_best.pt')\n",
    "            best_val_loss = val_loss\n",
    "\n",
    "        if early_stopper(val_loss):\n",
    "            print('Early stopping!')\n",
    "            break\n",
    "        \n",
    "    print('Finished Training')\n",
    "\n",
    "    return(history)\n",
    "\n",
    "\n",
    "\n",
    "class Early_Stopping():\n",
    "\n",
    "    def __init__(self, patience = 3, min_delta = 0.00001):\n",
    "\n",
    "        self.patience = patience \n",
    "        self.min_delta = min_delta\n",
    "\n",
    "        self.min_val_loss = float('inf')\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "\n",
    "        # improvement\n",
    "        if val_loss + self.min_delta < self.min_val_loss:\n",
    "            self.min_val_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "        # no improvement            \n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter > self.patience:\n",
    "                return True\n",
    "            \n",
    "        return False\n",
    "    \n",
    "from matplotlib import colors\n",
    "\n",
    "def plot_scatter(x,y,targets):\n",
    "    cmap = colors.ListedColormap(['black', 'darkred', 'darkblue', \n",
    "                                  'darkgreen', 'yellow', 'brown', \n",
    "                                  'purple', 'lightgreen', 'red', 'lightblue'])\n",
    "    bounds=[0, 0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5,8.5,9.5]\n",
    "    norm = colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.scatter(x, y, c = targets, cmap=cmap, s = 1, norm=norm)\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT = 28\n",
    "WIDTH = 28\n",
    "NUM_CHANNELS = 1\n",
    "BATCH_SIZE = 32\n",
    "LATENT_SPACE_DIM = 2\n",
    "\n",
    "MODEL_PATH = 'autoencoder_models'\n",
    "\n",
    "train_online = True\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(vcpi_util)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor()]) \n",
    "\n",
    "full_dataset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "\n",
    "train_set, val_set = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size = BATCH_SIZE, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size = BATCH_SIZE)\n",
    "\n",
    "test_set = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE,\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets = next(iter(train_loader))\n",
    "\n",
    "vcpi_util.show_images(4,8, images, targets, full_dataset.classes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_space_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.econv1 = torch.nn.Conv2d(1, 32, 3, 2)\n",
    "        self.erelu1 = torch.nn.ReLU()\n",
    "        self.eb1 = torch.nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.econv2 = torch.nn.Conv2d(32, 64, 3, 2)\n",
    "        self.erelu2 = torch.nn.ReLU()\n",
    "        self.eb2 = torch.nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.efc1 = torch.nn.Linear(64*7*7, latent_space_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        \n",
    "        self.dfc1 = torch.nn.Linear(latent_space_dim, 64*7*7)\n",
    "        self.dconv1 = torch.nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1)\n",
    "        self.drelu1 = torch.nn.ReLU()\n",
    "        self.db1 = torch.nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.dconv2 = torch.nn.ConvTranspose2d(32, 1, 3, stride=2, padding=1, output_padding=1)\n",
    "        self.dsig = torch.nn.Sigmoid()\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        x = torch.nn.functional.pad(x, (0, 1, 0, 1))\n",
    "        x = self.econv1(x)\n",
    "        x = self.erelu1(x)\n",
    "        x = self.eb1(x)\n",
    "        \n",
    "        x = torch.nn.functional.pad(x, (0, 1, 0, 1))\n",
    "        x = self.econv2(x)\n",
    "        x = self.erelu2(x)\n",
    "        x = self.eb2(x)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = self.efc1(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def decoder(self, x):\n",
    "        \n",
    "        x = self.dfc1(x)\n",
    "        x = x.reshape(-1, 64, 7, 7)\n",
    "        x = self.dconv1(x)\n",
    "        x = self.drelu1(x)\n",
    "        x = self.db1(x)\n",
    "        \n",
    "        x = self.dconv2(x)\n",
    "        x = self.dsig(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def forward(self,x):\n",
    "        encoded = self.encoder(x)\n",
    "        result = self.decoder(encoded)\n",
    "        return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE = Autoencoder(LATENT_SPACE_DIM)\n",
    "AE.to(device)\n",
    "torchinfo.summary(AE, input_size=(BATCH_SIZE, NUM_CHANNELS, HEIGHT, WIDTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "opt = torch.optim.Adam(AE.parameters(), lr = 1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', factor=0.1, patience=3)\n",
    "early_stop = Early_Stopping(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train(AE, train_loader, val_loader, 100, loss_fn, opt, scheduler, early_stop, f'auto_{LATENT_SPACE_DIM}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload = torch.load(f'auto_{LATENT_SPACE_DIM}_best.pt')\n",
    "AE.load_state_dict(reload['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_preds(set1, set2, count):\n",
    "    \n",
    "    columns = 4\n",
    "    rows = int(count * 2 / columns) + 1\n",
    "    plt.figure(figsize=(count, 2*rows))\n",
    "    \n",
    "    for n in range(count):\n",
    "        ax = plt.subplot(rows, columns, n*2 + 1)\n",
    "        plt.title('original')\n",
    "        plt.imshow(np.transpose(set1[n].cpu().detach().numpy(), (1, 2, 0)), cmap= plt.cm.gray)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        ax = plt.subplot(rows, columns, n*2 + 2)\n",
    "        plt.title('recon')\n",
    "        plt.imshow(np.transpose(set2[n].cpu().detach().numpy(), (1, 2, 0)), cmap= plt.cm.gray)\n",
    "        plt.axis('off')\n",
    "\n",
    "i, _ = next(iter(test_loader))\n",
    "recon = AE(i.to(device))\n",
    "show_preds(i, recon, 10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = []\n",
    "targets = []\n",
    "\n",
    "for i,t in test_loader:\n",
    "    encoded.extend(AE.encoder(i.to(device)).cpu().detach().numpy())\n",
    "    targets.extend(t)\n",
    "    \n",
    "x = np.array(encoded)[:,0]\n",
    "y = np.array(encoded)[:,1]\n",
    "\n",
    "plot_scatter(x, y, targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = AE.decoder(torch.Tensor([[-4,0]]).to(device))\n",
    "plt.imshow(np.transpose(pred[0].cpu().detach().numpy(), (1, 2, 0)), cmap= plt.cm.gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 5\n",
    "steps = 20\n",
    "\n",
    "step = (2*limit)/steps\n",
    "vector_generation = [[-limit + j * step, limit - i * step] for i in range(steps) for j in range(steps)]\n",
    "\n",
    "predictions = AE.decoder(torch.Tensor(vector_generation).to(device))\n",
    "\n",
    "vcpi_util.show_predicted_images(steps, steps, predictions.cpu().detach(),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def show_anomaly_sample(image, recon, err):\n",
    "    \n",
    "    plt.figure(figsize=(5,2))\n",
    "    alx = plt.subplot(1, 2, 0)\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.title(str(err))\n",
    "    ax = plt.subplot(1, 2, 1)\n",
    "    plt.imshow(recon, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    \n",
    "def anomalyDetection(image_path):\n",
    "    \n",
    "    files = glob.glob(f'{image_path}/*.jpg')\n",
    "    for f in files:\n",
    "\n",
    "        img = Image.open(f).convert('L').resize((28,28))\n",
    "        data = transform(img).to(device).view(1,1,28,28)\n",
    "        \n",
    "        testing = AE(data)\n",
    "        err = np.mean(img - testing.cpu().detach().numpy())\n",
    "        show_anomaly_sample(img, np.transpose(testing.cpu().detach().numpy(), (1, 2, 0)), err)\n",
    "\n",
    "image_path = 'anomalyDetectionImages'\n",
    "anomalyDetection(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "\n",
    "for i in range(train_set.__len__()):\n",
    "    features.extend(AE.encoder(train_set[i][0].view(1,1,28,28).to(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = list(range(0, train_set.__len__()))\n",
    "data = {'indexes':indexes, 'features':features}\n",
    "\n",
    "def show_content_retrieval(train_X, top50):\n",
    "    fig = plt.figure(figsize=(10, 50))\n",
    "    for i in range(len(top50)):\n",
    "        ax = plt.subplot(25, 5, i+1)\n",
    "        plt.imshow(np.transpose(train_X[top50[i][1]][0], (1, 2, 0)))\n",
    "        plt.title(f'{top50[i][0]:.3f}')\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'contentRetrievalImages'\n",
    "\n",
    "img = Image.open(image_path).convert('L').resize((28,28))\n",
    "img_tensor = transform(img).to(device).view(1,1,28,28)\n",
    "\n",
    "dataLatent = AE.encoder(img_tensor).cpu().detach().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in range(len(data['indexes'])):\n",
    "    err = np.sum((dataLatent - data['features'][i].cpu().detach().numpy()) ** 2)\n",
    "    err /= float(dataLatent.shape[0])\n",
    "    \n",
    "    results.append([err, i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top50 = sorted(results)[:50]\n",
    "\n",
    "show_content_retrieval(train_set, top50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

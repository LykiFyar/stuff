{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ficha de Análise Léxica (Lex)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A análise léxica é o processo de conversão de uma sequência de caracteres numa sequência de *tokens*, em que cada *token* representa uma unidade significativa da linguagem à qual os caracteres pertencem.\n",
    "\n",
    "Em Python, podemos fazer análise léxica de várias formas. A que iremos utilizar nas aulas recorre ao módulo Ply, que para além de análise léxica vai-nos permitir fazer análise sintática.\n",
    "\n",
    "Antes de usar o módulo Ply, precisamos de o instalar. Para isso, podemos usar o comando seguinte:\n",
    "\n",
    "```sh\n",
    "$ pip install ply\n",
    "```\n",
    "\n",
    "Depois, apenas precisamos de importar a ferramenta `lex.py` no nosso programa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ply.lex as lex"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A primeira coisa que o nosso analisador léxico (ou *lexer*/*tokenizer*) precisa de ter é uma lista de *tokens*. Como exemplo, vamos definir um *lexer* que lê expressões aritméticas, como \"4 * (2 + 3)\". Neste exemplo já somos capazes de identificar alguns *tokens*..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = (\n",
    "    'NUMBER',\n",
    "    'PLUS',\n",
    "    # completar...\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguir é preciso especificar cada *token*. Por outras palavras, precisamos de definir expressões regulares que permitam ao *tokenizer* identificar os *tokens*. Podemos fazê-lo através de variáveis ou de funções."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_PLUS = r'\\+'\n",
    "# completar...\n",
    "\n",
    "def t_NUMBER(t):\n",
    "    # completar\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos especificar um conjunto de caracteres que o analisador léxico vai ignorar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_ignore = ' \\t\\n'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precisamos ainda de definir o comportamento do *tokenizer* caso encontre um carácter ou sequência de caracteres que não corresponda a nenhum *token* conhecido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_error(t):\n",
    "    print(f\"Carácter ilegal {t.value[0]}\")\n",
    "    t.lexer.skip(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, já somos capazes de construir o nosso analisador léxico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexer = lex.lex()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para o usar, precisamos de lhe dar algum valor de *input* e depois pedir-lhe para ir devolvendo os *tokens* que encontrar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '''\n",
    "3 + 4 * 10\n",
    "  + -20 *2\n",
    "'''\n",
    "\n",
    "lexer.input(data)\n",
    "\n",
    "while True:\n",
    "    tok = lexer.token()\n",
    "    if not tok:\n",
    "        break\n",
    "    print(tok)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É possível consultar a documentação do *lex.py* em https://ply.readthedocs.io/en/latest/ply.html#lex. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercícios"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Frases\n",
    "\n",
    "Define um analisador léxico capaz de ler uma frase e de identificar os seus componentes (palavras, vírgulas, sinais de pontuação)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ON FILE: ex1_fichaLex.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Listas Mistas\n",
    "\n",
    "Define um analisador léxico capaz de receber listas números, palavras ou valores booleanos como input (e.g.: `[ 1,5, palavra, False ,3.14,   0, fim]`) e identificar os seus *tokens*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ON FILE: ex2_fichaLex.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. JSON\n",
    "\n",
    "Define um analisador léxico capaz de ler ficheiros em formato JSON e identificar os seus *tokens*.\n",
    "\n",
    "Exemplo de um documento JSON:\n",
    "\n",
    "---\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"name\": \"John Doe\",\n",
    "  \"age\": 21,\n",
    "  \"gender\": \"male\",\n",
    "  \"height\": 1.68,\n",
    "  \"address\": {\n",
    "    \"street\": \"123 Main Street\",\n",
    "    \"city\": \"New York\",\n",
    "    \"country\": \"USA\",\n",
    "    \"zip\": \"10001\"\n",
    "  },\n",
    "  \"married\": false,\n",
    "  \"hobbies\": [\n",
    "    {\n",
    "      \"name\": \"reading\",\n",
    "      \"books\": [\n",
    "        {\n",
    "          \"title\": \"Heartstopper: Volume 1\",\n",
    "          \"author\": \"Alice Oseman\",\n",
    "          \"genres\": [\"Graphic Novels\", \"Romance\", \"Queer\"]\n",
    "        },\n",
    "        {\n",
    "          \"title\": \"1984\",\n",
    "          \"author\": \"George Orwell\",\n",
    "          \"genres\": [\"Science Fiction\", \"Dystopia\", \"Politics\"]\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"gaming\",\n",
    "      \"games\": [\n",
    "        {\n",
    "          \"title\": \"Portal 2\",\n",
    "          \"platform\": [\"PC\", \"PlayStation 3\", \"Xbox 360\"]\n",
    "        },\n",
    "        {\n",
    "          \"title\": \"Synth Riders\", \n",
    "          \"platform\": [\"PSVR\", \"PSVR2\", \"PCVR\", \"Oculus Quest\"]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Condições de contexto\n",
    "\n",
    "Para certos analisadores léxico, pode ser útil ter diferentes estados. Por exemplo, se definirmos um analisador léxico para um ficheiro XML, pode ser útil verificar se o nome usado para fechar uma *tag* foi o mesmo que foi usado para a abrir.\n",
    "\n",
    "Exemplo de parte de um ficheiro XML:\n",
    "\n",
    "```xml\n",
    "<pessoa>\n",
    "    <nome>Maria</nome>\n",
    "    <idade>32</idade>\n",
    "</pessoa>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ply.lex as lex\n",
    "\n",
    "states = (\n",
    "    ('taga', 'exclusive'),\n",
    "    ('tagf', 'exclusive'), # num estado exclusivo, apenas aplicamos os tokens e regras para esse estado\n",
    "                           # por outro lado, num estado inclusivo, as regras e tokens desse estado juntam-se às outras regras e tokens\n",
    "                           # o estado inicial chama-se 'INITIAL' e não é preciso defini-lo\n",
    ")\n",
    "\n",
    "tokens = (\n",
    "    'ABRIR_TAG',\n",
    "    'ABRIR_TAG_F',\n",
    "    'FECHAR_TAG',\n",
    "    'NOME_TAG',\n",
    "    'VALOR'\n",
    ")\n",
    "\n",
    "t_ignore = ' \\t\\n' # estes tokens apenas são ignorados no estado 'INITIAL' e em estados inclusivos\n",
    "\n",
    "t_VALOR = r'[^<]+'\n",
    "\n",
    "def t_ABRIR_TAG_F(t):\n",
    "    r'</'\n",
    "    t.lexer.begin('tagf') # entramos no estado 'tagf'\n",
    "    return t\n",
    "\n",
    "def t_ABRIR_TAG(t):\n",
    "    r'<'\n",
    "    t.lexer.begin('taga') # entramos no estado 'taga'\n",
    "    return t\n",
    "\n",
    "def t_taga_tagf_FECHAR_TAG(t):\n",
    "    r'>'\n",
    "    t.lexer.begin('INITIAL') # voltamos ao estado inicial\n",
    "    return t\n",
    "\n",
    "def t_taga_NOME_TAG(t):\n",
    "    r'\\w+'\n",
    "    t.lexer.stack.append(t.value)\n",
    "    return t\n",
    "\n",
    "def t_tagf_NOME_TAG(t):\n",
    "    r'\\w+'\n",
    "    if len(t.lexer.stack) > 0:\n",
    "        if (nt := t.lexer.stack.pop(-1)) != t.value:\n",
    "            print(f\"Erro - esperado nome de tag '{nt}', mas foi lido '{t.value}'!\")\n",
    "    else:\n",
    "        print(\"Erro - nenhuma tag aberta!\")\n",
    "    return t\n",
    "\n",
    "def t_ANY_error(t): # regra válida para todos os estados\n",
    "    print(f\"Carácter ilegal: {t.value[0]}\")\n",
    "    t.lexer.skip(1)\n",
    "\n",
    "\n",
    "data = '''\n",
    "<pessoa>\n",
    "    <nome>Maria</nome>\n",
    "    <idade>32</idade>\n",
    "</pessoa>\n",
    "'''\n",
    "\n",
    "lexer = lex.lex()\n",
    "\n",
    "lexer.stack = list() # vamos usar esta lista como stack para verificar os nomes das tags\n",
    "\n",
    "lexer.input(data)\n",
    "\n",
    "while True:\n",
    "    tok = lexer.token()\n",
    "    if not tok:\n",
    "        break\n",
    "    print(tok)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se quisermos manter informação sobre as linhas nas quais os *tokens* aparecem, podemos usar o atributo `lineno`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_ignore = r' \\t'\n",
    "\n",
    "def t_newline(t):\n",
    "    r'\\n+'\n",
    "    t.lexer.lineno += len(t.value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercícios 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. BibTeX\n",
    "\n",
    "Define um analisador léxico capaz de ler um ficheiro no formato *BibTeX* e identificar os seus *tokens*.\n",
    "\n",
    "Exemplo de um ficheiro BibTeX:\n",
    "\n",
    "---\n",
    "\n",
    "```bibtex\n",
    "@incollection {HDYE78,\n",
    "author = \"Ricardo Martini and Pedro Rangel Henriques and Giovani Libreloto\",\n",
    "title = \"Storing Archival Emigration Documents to Create Virtual Exhibition Rooms\",\n",
    "booktitle = \"New Contributions in Information Systems and Technologies\",\n",
    "series=\"Advances in Intelligent Systems and Computing\",\n",
    "editor=\"Rocha, Alvaro and Correia, Ana and Costanzo, S. and Reis, Luis Paulo\",\n",
    "volume=\"353\",\n",
    "pages=\"403-409\",\n",
    "year = \"2015\",\n",
    "month =  \"April\"\n",
    "}\n",
    "\n",
    "\n",
    "@book {H787,\n",
    "author = {Vitor T. Martins and Pedro Rangel Henriques and Daniela da Cruz},\n",
    "title = {An AST-based tool, Spector, for Plagiarism Detection},\n",
    "booktitle = {Proceedings of SLATE’15},\n",
    "pages = {173--178},\n",
    "ISBN = {},\n",
    "year = {2015},\n",
    "month =   {},\n",
    "publisher = {Fundacion General UCM},\n",
    "annote = {Keywords: software, plagiarism, detection, comparison, test}}\n",
    "\n",
    "@book {H787,\n",
    "author = {Vitor T. Martins and Pedro Rangel Henriques and Daniela da Cruz},\n",
    "title = \"{A}n {AST}-based tool, {S}pector, for Plagiarism Detection\",\n",
    "booktitle = {Proceedings of SLATE’15},\n",
    "pages = {173--178},\n",
    "ISBN = {},\n",
    "year = {2015},\n",
    "month =   {},\n",
    "publisher = {Fundacion General UCM},\n",
    "annote = {Keywords: software, plagiarism, detection, comparison, test}\n",
    "}\n",
    "```\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Markdown\n",
    "\n",
    "Define um analisador léxico capaz de ler ficheiros em formato Markdown e identificar os seus *tokens*. O *tokenizer* deve conseguir identificar pelo menos:\n",
    "\n",
    "- cabeçalhos\n",
    "- parágrafos\n",
    "- listas\n",
    "- texto itálico\n",
    "- texto negrito\n",
    "- blocos de código\n",
    "- citações\n",
    "\n",
    "Exemplo de um documento Markdown:\n",
    "\n",
    "---\n",
    "\n",
    "````markdown\n",
    "# This is a heading\n",
    "\n",
    "## This is a subheading\n",
    "\n",
    "This is some **bold** text.\n",
    "\n",
    "This is some *italic* text.\n",
    "\n",
    "- This is a bullet point\n",
    "- This is another bullet point\n",
    "\n",
    "1. This is a numbered list\n",
    "2. This is another numbered list item\n",
    "\n",
    "> This is a blockquote.\n",
    "\n",
    "`This is some inline code.`\n",
    "\n",
    "```python\n",
    "# This is some code block\n",
    "print(\"Hello, world!\")\n",
    "```\n",
    "````\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOT COMPLETE, on file exMD_fichaLex.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Somador on/off\n",
    "\n",
    "Cria um programa em Python que tenha o seguinte comportamento:\n",
    "\n",
    "* Pretende-se um programa que some todas as sequências de dígitos que encontre num texto;\n",
    "* Prepara o programa para ler o texto do canal de entrada: stdin;\n",
    "* Sempre que encontrar a string “Off” em qualquer combinação de maiúsculas e minúsculas, esse comportamento é desligado;\n",
    "* Sempre que encontrar a string “On” em qualquer combinação de maiúsculas e minúsculas, esse comportamento é novamente ligado;\n",
    "* Sempre que encontrar o caráter “=”, o resultado da soma é colocado na saída.\n",
    "\n",
    "Este exercício já foi proposto como TPC, mas agora deves tentar resolvê-lo usando um analisador léxico com condições de contexto."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5994ec7557f9f4a3c4b5d4f7132f657ee5f793fd2b9e6534077474582be4b489"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
